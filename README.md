# Data Fetching Project 

## ğŸ”° Overview

Simple CLI-run program which calculates the best price for a given amount of 
BTC. It implements dollar cost average approach splitting the total amount
on multiple transactions and checking the best available price for each
across multiple exchanges.

## ğŸ”§ Tech Stack
- Python 3.12 - Primary programming language

## ğŸ“Š The Data

The price information is collected from 3 crypto exchanges using REST API and 
saved locally.
I'm currently working on my next project using WebSocket. 

## ğŸ“ File Structure
```
Data-Fetching-project
â”œâ”€â”€ src                     
â”‚   â”œâ”€â”€ exchanges
â”‚   â”‚   â”œâ”€â”€ base.py              # Defines class Exchanges
â”‚   â”‚   â”œâ”€â”€ binance.py           # Defines subclass Binance
â”‚   â”‚   â”œâ”€â”€ coinbase.py          # Defines subclass Coinbase
â”‚   â”‚   â””â”€â”€kraken.py             # Defines subclass Kraken
â”‚   â”œâ”€â”€ utils
â”‚   â”‚   â”œâ”€â”€ get_best_price.py    # Calculates best available price
â”‚   â”‚   â””â”€â”€helpers.py            # Miscellaneous util functions
â”œâ”€â”€ main.py                      # Execution script
â”œâ”€â”€ data                         
â”‚   â””â”€â”€ best_deal.csv            # Record of best price for each API call
â”œâ”€â”€ .gitignore                   # Files not to be pushed to remote repository
â”œâ”€â”€ README.md                    # Project overview
â””â”€â”€ requirements.txt             # Third party Python modules
```

## â–¶ï¸ How to run
Run in CLI. Following input is required:
ğŸ’  Amount of BTC to acquire
ğŸ’  Time to complete the purchase
ğŸ’  Number of transactions (opt)

## â—€ï¸ Output
In CLI:
ğŸ’  Best total price for required amount
Data:
ğŸ’  Best total price for required amount








Project overview (1â€“2 lines)

What problem it solves (why best price matters)

Tech stack

How to run it

Example output

Folder structure

What's next (e.g., youâ€™re working on a websocket version)



# Data Engineering Project - Terrific Totes



## ğŸ”° Overview
We have been approached by Terrific Totes to create a data-pipeline to Extract, Transform, and Load data from a prepared source into a data lake and warehouse hosted in AWS.

We will deliver a data platform that extracts data from an operational OLTP database, archives it in a data lake, and makes it available in a remodelled OLAP data warehouse.

## ğŸ”§ Tech Stack
- Github - Repository management, CI/CD (Github-Actions), Credentials Security (Github-Secrets)
- AWS - RDS, Lambda, CloudWatch, S3
- Terraform - AWS Deployment (Infrastructure as Code)
- Python 3.12 - Primary programming language
- Pytest - Test Driven Development (TDD)
- PostgreSQL - Relational Database Management


## ğŸ›ï¸ Architecture
- Two S3 buckets (one for ingested data and one for processed data). Both buckets are structured and well-organised so that data is easy to find.
- The Python application continually ingests all tables from the `totesys` database and stores the injested data in a json format. The application also:
  - operates automatically on a schedule
  - logs progress to Cloudwatch
  - triggers email alerts in the event of failures
  - follows good security practices (for example, preventing SQL injection and maintaining password security)
- The Python application remodels the data into a predefined schema suitable for a data warehouse and stores the data in Parquet format. The application also:
  - triggers automatically when it detects the completion of an ingested data job
  - adequately logs and monitors
  - populates the dimension and fact tables of a single "star" schema in the warehouse.
- The Python application loads the data into a prepared data warehouse at intervals. This is also logged and monitored.
- Includes a visual presentation that allows users to view useful data in the warehouse.

All Python code is thoroughly tested, PEP8 compliant, and tested for security vulnerabilities with the `pip-audit` and `bandit` packages. Test coverage exceeds 90%.

The project is deployed automatically using infrastucture-as-code (Terraform) and CI/CD (Github-Actions).

Changes to the source database will be reflected in the data warehouse within 30 minutes.

## ğŸ“Š The Data

The primary data source for the project is a database called `totesys` which is meant to simulate the back-end data of a commercial application. Data is inserted and updated into this database several times a day.


The data is remodelled into three overlapping star schemas. You can find the ERDs for these star schemas:

- ["Sales" schema](https://dbdiagram.io/d/637a423fc9abfc611173f637)
- ["Purchases" schema](https://dbdiagram.io/d/637b3e8bc9abfc61117419ee)
- ["Payments" schema](https://dbdiagram.io/d/637b41a5c9abfc6111741ae8)

The overall structure of the resulting data warehouse is shown [here](https://dbdiagram.io/d/63a19c5399cb1f3b55a27eca).

## ğŸ‘€ Visualisation

We have created a BI dashboard to visualise some data insights:

- TODO

## ğŸ“ File Structure

```
FSCIFA-project
â”œâ”€â”€ .github
â”‚   â””â”€â”€ workflows
â”‚       â””â”€â”€ ci.yml          # CI/CD Automated deployment via Github Actions
â”œâ”€â”€ dependencies_db/        # Python dependencies for db connection
â”œâ”€â”€ src                     # Source code for ETL/ELT
â”‚   â”œâ”€â”€ python
â”‚   â”‚   â”œâ”€â”€ db              # Python database connection functions
â”‚   â”‚   â””â”€â”€ utils           # Python utility functions
â”‚   â”œâ”€â”€ extract_lambda.py   # ETL - Extract lambda function
â”‚   â”œâ”€â”€ load_lambda.py      # ETL - Load lambda function
â”‚   â””â”€â”€ transform_lambda.py # ETL - Transform lambda function
â”œâ”€â”€ tests/                  # Unit and integration tests
â”‚   â”œâ”€â”€ data/               # Sample data or data schemas for tests
â”‚   â””â”€â”€ test*.py            # Unit and integration tests for python functions (pytest)
â”œâ”€â”€ terraform/              # AWS Deployment
â”œâ”€â”€ .gitignore              # Files not to be pushed to remote repository
â”œâ”€â”€ Makefile                # Automated environment setup & configuration
â”œâ”€â”€ mvp.png                 # Illustration of expected minimum viable product
â”œâ”€â”€ README.md               # Project overview
â”œâ”€â”€ requirements_db.txt     # Third party Python modules for db connection
â””â”€â”€ requirements.txt        # Third party Python modules
```

## ğŸš€ Setup & Deployment

This project uses GitHub Actions for continuous integration and deployment, the workflow automatically runs tests and deploys AWS infrastructure via Terraform.

The CI/CD pipeline is triggered on:
  - Pushes to the main branch
  - Pull requests targeting the main branch


### Continuous Integration  
The run-tests job performs the following steps:

 - Configures the Python environment and installs dependancies
 - Runs python security, format and linting checks
 - Runs pytests and checks test coverage

### Terraform Deployment
The deploy-terraform job runs only after successful tests and performs the following:

- Installs Terraform
- Runs Terraform Init, Plan & Apply


### Required Secrets for AWS and Terraform deployment:

AWS credentials secrets:
 - DEPLOY_USER_AWS_ACCESS_KEY_ID
 - DEPLOY_USER_AWS_SECRET_ACCESS_KEY

Terraform variable secrets:
  - TF_VAR_pg_host
  - TF_VAR_pg_port
  - TF_VAR_pg_user
  - TF_VAR_pg_database
  - TF_VAR_pg_password
  - TF_VAR_dw_host
  - TF_VAR_dw_database
  - TF_VAR_dw_password



##
*A Northcoders Data Engineering Bootcamp Project*
